\documentclass[bachelor, och, pract]{SCWorks}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{listings}
\usepackage{array}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[english,russian]{babel}

\usepackage{listings}

\lstset{language=Swift}
\lstset{numbers = left}
\lstset{numberstyle = \tiny}
\lstset{basicstyle=\small}
\lstset{breaklines=true}
\lstset{extendedchars=true}
\lstset{breakatwhitespace=true}

\usepackage[colorlinks=true]{hyperref}

\newcommand{\eqdef}{\stackrel {\rm def}{=}}



\begin{document}
	
	% Кафедра (в родительном падеже)
	\chair{математической кибернетики и компьютерных наук}
	
	% Тема работы
	\title{Построение архитектуры  ETL процесса и ее программная реализация}
	
	% Курс
	\course{4}
	
	% Группа
	\group{411}
	
	% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
	%\department{факультета КНиИТ}
	
	% Специальность/направление код - наименование
	\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
	
	% Фамилия, имя, отчество в родительном падеже
	\author{Вязкова Андрея Андреевича}
	
	% Заведующий кафедрой
	\chtitle{к.\,ф.-м.\,н.} % степень, звание
	\chname{А.\,С.\,Иванов}
	
	%Научный руководитель (для реферата преподаватель проверяющий работу)
	\satitle{доцент, к.\,т.\,н.} %должность, степень, звание
	\saname{В.\,М.\,Соловьев}
	
	% Руководитель практики от организации (только для практики,
	% для остальных типов работ не используется)
	\patitle{к.\,ф.-м.\,н}
	\paname{С.\,В.\,Миронов}
	
	% Семестр (только для практики, для остальных
	% типов работ не используется)
	\term{8}
	
	% Наименование практики (только для практики, для остальных
	% типов работ не используется)
	\practtype{преддипломная}
	
	% Продолжительность практики (количество недель) (только для практики,
	% для остальных типов работ не используется)
	\duration{4}
	
	% Даты начала и окончания практики (только для практики, для остальных
	% типов работ не используется)
	\practStart{30.04.2020}
	\practFinish{27.05.2020}
	
	% Год выполнения отчета
	\date{2020}
	
	\maketitle
	
	% Включение нумерации рисунков, формул и таблиц по разделам
	% (по умолчанию - нумерация сквозная)
	% (допускается оба вида нумерации)
	%\secNumbering
	
	
	\tableofcontents
	
% Раздел "Введение"
\intro

Большие данные являются популярным трендом в современном IT. С помощью них можно прогнозировать события, угадывать поведение пользователя, обнаруживать мошенническую активность. Большие данные используются в таких отраслях как: финансовые услуги, здравоохранение, физика.

Однако не всегда данные могут быть однородными и готовыми для дальнейшего анализа. Чтобы они были готовы их необходимо предварительно обработать. Подготовка данных для дальнейшего анализа называется  ETL процессом от английского Extract "--- извлечение, Transform "--- трансформация, Load "--- загрузка. В данной работе показан пример построения архитектуры этого процесса с помощью современных инструментов в облаке Amazon Web Services.

\section{Большие данные}

\subsection{Основные понятия}

Большие данные "--- обозначение структурированных и неструктурированных данных огромных объёмов и значительного многообразия, эффективно обрабатываемых горизонтально масштабируемыми программными инструментами.

Термин <<Большие Данные>> \ref{habr1}  вызывает множество споров, многие полагают, что он означает лишь объем накопленной информации, но не стоит забывать и о технической стороне. К данной сфере именно относится обработка и хранение  большого объема информации, для которого традиционные способы являются неэффективными, а также сервисные услуги. В таблице \ref{tab1} представлена сравнительная характеристика традиционной базы данных и базы больших данных.

\begin{table}[h!]
	\caption{Сравнительный анализ традиционной базы данных и базы Больших данных}
	\label{tab1}
	\begin{center}
		\begin{tabular}{|p{0.3\linewidth}|p{0.3\linewidth}|p{0.3\linewidth}|}
			\hline
			Характеристика & Традиционная БД &База больших данных \\
			\hline
			Объем информации & От гигабайт до терабайт & От петабайт до эксабайт\\
			\hline
			Способ хранения & Централизованный& Децентрализованный \\
			\hline
			Структурированность данных & Структурирована & Неструктурирована или полуструктурирована \\
			\hline
			Модель хранения и обработки данных & Вертикальная модель & Горизонтальная модель \\
			\hline
			Взаимосвязь данных & Сильная & Слабая \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

Большие данные характеризуются следующими признаками:

\begin{itemize}
	\item \textbf{Volume} "--- объем, накопленная база данных представляет собой большой объем информации, который трудно хранить и обрабатывать традиционными способами;
	\item \textbf{Velocity} "--- скорость, данный признак указывает как на увеличивающуюся скорость накопления данных, так и на скорость обработки данных, в последнее время стали более востребованы технологии обработки данных в реальном времени.
	\item \textbf{Variety} "--- многообразие, возможность одновременной обработки структурированной и неструктурированной разноформатной информации. Главное отличие структурированной информации – это то, что она может быть классифицирована. 
	Неструктурированная информация включает в себя видео, аудио файлы, свободный текст, информацию, поступающую из социальных сетей. Данная информация нуждается в комплексной обработке для дальнейшего анализа. 
	\item \textbf{Veracity} "--- достоверность, насколько точны полученные данные? \ref{Silen}
\end{itemize}

\subsection{Отрасли применения}

Большие Данные получили широкое распространение во многих отраслях. Их используют в здравоохранении, телекоммуникациях, торговле, логистике, в финансовых компаниях, а также в государственном управлении. Ниже представлены примеры их применения:

\begin{itemize}
	\item Финансовые услуги "--- большие данные дают возможность проанализировать кредитоспособность заемщика. Внедрение технологий Больших Данных позволит сократить время рассмотрения кредитных заявок. С помощью Больших Данных можно проанализировать операции конкретного клиента и предложить подходящие именно ему банковские услуги;
	\item Телекоммуникации "--- в этой отрасли большие данные получили популярность у сотовых операторов. Операторы сотовой связи наравне с финансовыми организациями имеют одни из самых объемных баз данных, что позволяет им проводить наиболее глубокий анализ накопленной информации. Главной целью анализа данных является удержание существующих клиентов и привлечение новых;
	\item Горнодобывающая и нефтяная промышленности "--- большие данные используются как при добыче полезных ископаемых, так и при их переработке и сбыте. Предприятия могут на основании поступившей информации делать выводы об эффективности разработки месторождения, отслеживать график капитального ремонта и состояния оборудования, прогнозировать спрос на продукцию и цены;
\end{itemize}

\subsection{Технологии  Больших данных}

Для сбора и обработки больших данных используются как известные технологии и концепции, так и достаточно новые. Вот некоторые из них:

\begin{itemize}
	\item \textbf{SQL} (Structured Query Language) "--- структурированный язык запросов для работы с базами данных. С его помощью можно манипулировать данными, а за управление данными отвечает движок базы данных и система управления базами данных;
	\item \textbf{NoSQL} (Not Only SQL) "--- совокупность подходов, направленных на создание базы данных, отличной от реляционной. Такой подход удобно использовать при меняющейся структуре данных;
	\item \textbf{MapReduce} "--- модель программирования распределенных вычислений над большими объемами данных, представленная компанией Google в 2004 году.  Весь процесс делится на две фазы: map и reduce. Все вычисления производятся на кластере из главного узла (master node) и рабочих узлов (worker nodes) Каждая из фаз принимает на вход и отдает список пар ключ/значение. Фаза map принимает список, главный узел разбивает этот список на части отправляет на каждый рабочий узел. Каждый рабочий узел применяет заранее написанную функцию к каждому элементу такого списка. Фаза reduce группирует значений по ключам и отправку результатов на главный узел. Также между этими фазами есть промежуточная фаза, называемая  shuffle. На этой фазе данные перераспределяются между рабочими узлами на основе ключей, таким образом, чтобы все данные по одному ключу лежали на одном рабочем узле.
	\item \textbf{Apache Spark} "--- фреймворк для быстрых кластерных вычислений, разработанный в Университете Беркли и переданный в фонд Apache. Распространяется по открытой лицензии. Spark использует парадигму резидентных вычислений. Все данные хранятся и обрабатываются в оперативной памяти. Использует ленивые вычисления. Основной концепцией в Spark является RDD (Resilient Distributed Dataset)\ref{habr:piter} "--- неизменяемая коллекция объектов, которую можно обрабатывать параллельно. RDD создается путем загрузки внешнего набора данных или распределения коллекции из основной программы. RDD поддерживает 2 типа операций\ref{spark:ops}: трансформации "--- операции над RDD, результатом которых является новый RDD и действия "--- операции, которые возвращают значения вычислений над RDD. Ядро Spark состоит из высокоуровневых библиотек, таких как: Spark SQL "--- обертка над SQL, позволяющаяя делать запросы в БД из кода, Spark Streaming "--- библиотека для потоковой обработки данных в реальном времени, MLlib "--- библиотека машинного обучения, GraphX "--- это библиотека для манипуляций над графами и выполнения с ними параллельных операций. Spark имеет программные интерфейсы на языках Scala, Java, Python, R.
\end{itemize}

Также существуют сервисы которые предоставляют готовые услуги по предоставлению баз данных и вычислительных ресурсов. Такие компании как  Amazon, Google и Microsoft предлагают подобные решения. Подробнее об этих решениях будет описано в разделе 2.

\subsection{Открытые данные}

\newpage

\section{ETL процесс. Его задачи и технологии}
ETL (Extract, Transform, Load) "--- одна из главных процедур копирования данных из одного или нескольких источников в конечную систему. Данные в конечной системе имеют общий структурированный вид. Термин набрал популярность в  1970"=х годах и преимущественно используется при построении хранилищ данных.\ref{en_wiki:etl}.

ETL процесс состоит из трех фаз. Ниже представлено описание каждой из них\ref{habr:etl}:
\begin{itemize}
	\item Extract "--- извлечение данных из различных источников. Источниками могут выступать: результаты работы программ, логи этих программ, копии таблиц базы данных, любой внешний набор данных;
	\item Transform "--- выполнение преобразований над данными, их фильтрация, группировка и агрегация. На этом этапе сырые данные превращаются в готовый для анализа датасет;
	\item Load "--- загрузка обработанных данных в место конечного использования, например в хранилище данных. Эти данные могут быть использованы конечными пользователями или их можно подать на вход другому ETL процессу.
\end{itemize}

ETL придает данным значительную ценность. Это не просто копирование данных из исходного источника в хранилище данных.
ETL решает такие проблемы как:
\begin{itemize}
	\item Удаляет ошибки  и исправляет недостающие данные;
	\item Настраивает данные из нескольких источников для совместного использования:
	\item Структурирует данные для использования конечными инструментами:
\end{itemize}

\subsection{Технологии для решения задач ETL}
Так как  ETL "--- весьма комплексная структура, то к ней выдвигаются определенные требования.

Хранилища данных должны иметь копию исходных данных, подтверждение транзакций, которые изменили данные, а также подтверждение безопасности копий данных с течением времени.
Исходные данные должны быть качественными. Над некачественными данными придется проводить слишком много манипуляций, чтобы они были пригодны для дальнейшего использования в ETL. Поэтому необходимо производить профилирование данных. Профилирование данных - это систематическое исследование качества, объема и контекста источника данных, позволяющее построить  ETL процесс. Если источник данных содержит некачественные данные,то для его пригодности может потребоваться:

\begin{itemize}
	\item Удаление некоторых полей полностью;
	\item Автоматическая замена поврежденных значений;
	\item Разработка нормализованного представления данных;
\end{itemize}

Управление и мониторинг процесса ETL "--- очень важно понимать каким образом будут строиться ETL пайплайны (от англ. pipeline "--- трубопровод). Будет ли разработчик конструировать их через пользовательский интерфейс или же писать код? Как хорошо хорошо инструмент реализует мониторинг и оповещение об ошибках? Реализован ли процесс перезапуска в случае ошибки? Существует ли возможность заново обработать данные, которые уже были обработаны?

Обработка данных "--- будет ли использован JVM"=ориентированный язык (например Java или Scala) или будет использован SQL (например Hive). Плюсы первого подхода заключаются в использовании парадигмы MapReduce, а также в том, что с помощью этого подхода удобно писать пользовательские функции, минусы подхода в том, что придется изучать один из языков. Во втором подходе весь процесс происходит вокруг SQL таблиц, что весьма удобно. Минусы подхода заключаются в том, что для того чтобы написать пользовательские функции придется задействовать сторонний язык программирования.

В данном разделе будут описаны технологии и инструменты для решения и реализации той или иной задачи.

\subsubsection{Хранилища данных}
\textbf{Amazon Redshift}\ref{redshift} "--- сервис управления хранилищами данных в облаке, представленный компанией Amazon в 2012 году. Представляет собой кластер из узлов, который состоит из узла@=лидера и одного или нескольких вычислительных узлов. Тип и количество вычислительных узлов, которые нужны, зависят от размера данных, количества запросов, которые будут выполняться, и требуемой производительности выполнения запроса. Позволяет добавляет и удалять узлы без остановки кластера. Может хранить объемы данных от сотен гигабайт до нескольких сотен петабайт. Работает на измененном движке PostgreSQL. Amazon Redshift позволяет делать резервные копии, как в ручном так и в автоматическом режиме. Также  имеется шифрование данных и защищенный доступ по SSL. Данные хранятся на каждом узле в блоках, которые называются срезами, используется колоночное хранение. Redshift \ref{habr:redshift:arch} использует архитектуру MPP (Massively Parallel Processing), разбивая большие наборы данных на куски, которые назначаются срезами в каждом узле. Быстрая производительность достигается параллельной обработкой каждого среза на узле. Главный узел объединяет результаты и возвращает их клиентскому приложению.

\textbf{Apache Hive}\ref{hive} "---хранилище данных создан компанией Facebook в 2010 году, затем передан фонду Apahe под открытой лицензией. Входит в экосистему Hadoop. Имеет собственный SQL диалект, назывемый HiveQL. HiveQL имеет  SQL запросы, выполняемые в Hive конвертируются в MapReduce  программы. Есть возможность создания пользовательских функций. Данные таблиц хранятся в HDFS. Поддерживаются следующие форматы:
\begin{enumerate}
	\item Текстовый файл "--- данные располагаются в строках, каждая строка является записью. Строки заканчиваются символом новой строки в  UNIX формате.
	\item  Последовательные файлы "---  кодируются как ключ и значение для каждой записи. Записи хранятся в двоичном формате и, следовательно, занимают меньше места, чем текстовый формат:
	\item Колоночный формат (RCFile,  Apache Parquet) "--- позволяет хранить значения столбцов рядом друг с другом и все метаданные об этой таблице:
	\item Avro файлы "--- формат кодирует схему своего содержимого непосредственно в файле, что позволяет пользователю самостоятельно сохранять сложные объекты. Avro не только  формат файла, но и структура сериализации и десериализации;
	\item Файлы ORC - это формат файла столбцов Optimized Row, который обеспечивает высокоэффективный способ хранения данных Hive и преодолевает ограничения других форматов файлов Hive.
	\item  Пользовательские форматы файлов;
\end{enumerate}

\textbf{Google BigQuery}\ref{gc:bigquery} "--- хранилище данных, разработанное компанией Google в 2011 году. Архитектура BigQuery не требует настройки серверов. В качестве движка для выполнения запросов используется Dremel "--- разработанный в Google  и позволяющий сканировать миллиарды строк в секунду. Dremel использует архитектуру массивных параллельных запросов для сканирования данных в базовой системе управления файлами Colossus\ref{panoply:dwh}. Colossus распределяет файлы на блоки по 64Мб среди множества вычислительных ресурсов, называемых узлами, которые сгруппированы в кластеры. Dremel использует колоночную структуру данных, аналогичную Redshift. Древовидная архитектура отправляет запросы тысячам машин за считанные секунды.

\subsection{Планировщики ETL процессов}

\textbf{AWS Glue}\ref{rabota_s_bigdata} "--- бессерверный сервис для построения задач ETL, представленный компанией AWS в 2017 году, состоит из:

\begin{itemize}
	\item централизованное хранилище метаданных хранилищ данных (AWS Data Catalog);
	\item подсистему ETL, автоматически генерирующую код на Python или Scala, кото-
	рый описывает трансформацию данных;
	\item планировщик заданий;
	\item подсистему мониторинга и перезапуска.
\end{itemize}

Основное назначение сервиса "--- построение системы доставки данных из различных источников в централизованное хранилище данных. 

Архитектура AWS Glue представлена на рисунке \ref{glue} и состоит из следующих компонентов:

\begin{figure}[!ht]
	\centering
	\includegraphics[width=10cm]{glue.png}
	\caption{\label{glue}
		Архитектура AWS Glue}
\end{figure}

\begin{itemize}
	\item Выполняемые задания (jobs) "--- непосредственно рабочие процессы, производящие требуемое извлечение, трансформацию и загрузку данных из источников в назначения;
	\item Сборщик, или краулер (crawler) "--- программа, подключаемая к хранилищам данных, которая используется для наполнения каталога данных метаданными источников. В каталоге сборщику указывается хранилище данных, и он, определив схему источника, создает в каталоге соответствующий объект, называемый таблицей, поскольку все источники данных так или иначе могут быть описаны как таблицы. Вдобавок к описанию таблицы сборщик извлекает дополнительные метаданные, необходимые для построения сценария выполняемого задания ETL;
	\item Классификатор (classifier) "--- часть сборщика, определяющая схему данных источника. Поддерживает источники типа JSON, CSV, AVRO и XML. Кроме того, имеет возможность работать с реляционными базами данных, поддерживающими протокол JDBC;
	\item Сценарий трансформации данных "--- сценарий, выполняющий задачу трансформации и копирования, генерируемый AWS Glue или предоставляемый пользователем. Для описания задач поддерживаются языки Python и Scala.
	\item Триггер (trigger) "--- встроенный механизм запуска задания, работающий по расписанию или на основе перехваченного события. При срабатывании триггера происходит основной процесс ETL: на внутренних вычислительных ресурсах, содержащих фреймворк Apache Spark, выполняется сценарий трансформации данных, сгенерированный AWS Glue или предоставляемый пользователем.
	\item Сервер Notebook "--- интерактивный редактор кода, позволяющий писать программы на PySpark;
	\item Каталог данных (data catalog) "--- централизованное хранилище данных, которое содержит описания таблиц, сгруппированные в базы данных, выполняемых заданий и пр.
\end{itemize}


\newpage
\section{Подробное описание архитектуры}
\subsection{Описание архитектуры в общем виде}
\subsection{Сервер с исходными данными}
\subsection{Среда запуска Spark приложений}
\subsubsection{Кластер}
\subsubsection{Объектное хранилище}
\subsubsection{Хранилище данных}
\subsection{Apache Airflow}
\subsubsection{Мастер сервер}
\subsubsection{Хранилище метаданных}
\subsubsection{Kubernetes кластер}

\section{Программная реализация}
\subsection{Скрипты создания инфраструктуры}
\subsection{Пример Spark приложения}
\subsection{Скрипты транспортировки данных}
\subsection{Airflow DAG}
\subsection{Конфигурации}
\subsubsection{Общие тома для логов и DAGов}
\subsubsection{Kubeconfig}
\subsubsection{Конфигурацяи Airflow для Kubernetes кластера}
\subsubsection{Конфигурация Airflow}

% Раздел "Заключение"
\conclusion
Заключение
\bibliographystyle{gost780uv}
\bibliography{thesis}

\appendix
%\section{Java"=приложение для автоматизированных тестов серверной части}\label{api-test-app}
%\subsection{Класс ProfilePageTest}
%\VerbatimInput[fontsize=\small]{summerpractice/api-test/src/test/java/ru/summerpractice/ProfilePageTest.java}
%\subsection{Класс DataProviderProfile}
%\VerbatimInput[fontsize=\small]{summerpractice/api-test/src/main/java/ru/summerpractice/dataproviders/DataProviderProfile.java}

\end{document}
